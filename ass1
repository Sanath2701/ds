 Step 1: Create a file named log.txt with the following content:

2024-04-01_08:00:00 user1 LOGIN
2024-04-01_12:00:00 user1 LOGOUT
2024-04-01_09:00:00 user2 LOGIN
2024-04-01_11:00:00 user2 LOGOUT

 Step 2: Create a Python script max_login_windows.py with this code:

from datetime import datetime
from collections import defaultdict

# Step 1: Read the log file
with open("log.txt", "r") as file:
    logs = file.readlines()

# Step 2: Group entries by user
user_events = defaultdict(list)

for line in logs:
    timestamp, user, action = line.strip().split()
    user_events[user].append((timestamp, action))

# Step 3: Calculate total login duration for each user
user_durations = defaultdict(int)

for user, events in user_events.items():
    # Sort events by timestamp
    sorted_events = sorted(events)
    for i in range(0, len(sorted_events) - 1, 2):
        login_time = datetime.strptime(sorted_events[i][0], "%Y-%m-%d_%H:%M:%S")
        logout_time = datetime.strptime(sorted_events[i + 1][0], "%Y-%m-%d_%H:%M:%S")
        duration = (logout_time - login_time).total_seconds()
        user_durations[user] += int(duration)

# Step 4: Find the user with the maximum duration
max_user = max(user_durations.items(), key=lambda x: x[1])
print(f"‚úÖ User with maximum login duration: {max_user[0]} ({max_user[1]} seconds)")


WITH HADOOP


ssignment 1 (with Hadoop on Windows)
üîß Prerequisites
Install Hadoop on Windows:

Download Hadoop (standalone mode): https://github.com/cdarlint/winutils

Install Java JDK

Set environment variables:

pgsql
Copy
Edit
JAVA_HOME=C:\Path\To\Java
HADOOP_HOME=C:\Path\To\Hadoop
Add to PATH: %HADOOP_HOME%\bin
Place winutils.exe in %HADOOP_HOME%\bin

Create working folder structure (example):

lua
Copy
Edit
D:\hadoop_project\
‚îú‚îÄ‚îÄ input\
‚îÇ   ‚îî‚îÄ‚îÄ log.txt
‚îú‚îÄ‚îÄ mapper.py
‚îú‚îÄ‚îÄ reducer.py
‚îú‚îÄ‚îÄ output\
üìÑ Sample Log File: input/log.txt
text
Copy
Edit
2024-04-01_08:00:00 user1 LOGIN
2024-04-01_12:00:00 user1 LOGOUT
2024-04-01_09:00:00 user2 LOGIN
2024-04-01_11:00:00 user2 LOGOUT
üß† Mapper (mapper.py)
python
Copy
Edit
#!/usr/bin/env python3
import sys

for line in sys.stdin:
    parts = line.strip().split()
    if len(parts) == 3:
        timestamp, user, action = parts
        print(f"{user}\t{timestamp},{action}")
üß† Reducer (reducer.py)


#!/usr/bin/env python3
import sys
from collections import defaultdict
from datetime import datetime

logs = defaultdict(list)
durations = defaultdict(int)

for line in sys.stdin:
    user, value = line.strip().split('\t')
    timestamp, action = value.split(',')
    logs[user].append((timestamp, action))

for user, events in logs.items():
    sorted_events = sorted(events)
    for i in range(0, len(sorted_events)-1, 2):
        in_time = datetime.strptime(sorted_events[i][0], "%Y-%m-%d_%H:%M:%S")
        out_time = datetime.strptime(sorted_events[i+1][0], "%Y-%m-%d_%H:%M:%S")
        durations[user] += int((out_time - in_time).total_seconds())

# Output all durations, Hadoop will find the max if needed
for user, dur in durations.items():
    print(f"{user}\t{dur}")
‚öôÔ∏è Hadoop Commands (in CMD/PowerShell)
Navigate to your Hadoop installation folder and run:

bash
Copy
Edit
# Step 1: Remove old files from HDFS (if any)
hdfs dfs -rm -r /input
hdfs dfs -rm -r /output

# Step 2: Upload input log file to HDFS
hdfs dfs -mkdir /input
hdfs dfs -put D:\hadoop_project\input\log.txt /input/

# Step 3: Run Hadoop Streaming MapReduce Job
hadoop jar %HADOOP_HOME%\share\hadoop\tools\lib\hadoop-streaming-*.jar ^
  -input /input/log.txt ^
  -output /output ^
  -mapper "python mapper.py" ^
  -reducer "python reducer.py" ^
  -file D:\hadoop_project\mapper.py ^
  -file D:\hadoop_project\reducer.py
üì§ View the Output

hdfs dfs -cat /output/part-00000


